{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 100 new tweets\n",
      "Fetched 18 new tweets\n",
      "Fetched 20 new tweets\n",
      "Fetched 23 new tweets\n",
      "Fetched 19 new tweets\n",
      "Fetched 28 new tweets\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m os\u001b[39m.\u001b[39mmakedirs(directory, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     81\u001b[0m \u001b[39m# Start fetching and saving tweets\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m scraper\u001b[39m.\u001b[39;49mfetch_and_save(search_terms, count, filepath)\n",
      "Cell \u001b[0;32mIn[1], line 57\u001b[0m, in \u001b[0;36mTwitterScraper.fetch_and_save\u001b[0;34m(self, search_terms, count, filepath, interval, max_tweets)\u001b[0m\n\u001b[1;32m     55\u001b[0m total_tweets \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tweets)\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m total_tweets \u001b[39m<\u001b[39m max_tweets:\n\u001b[0;32m---> 57\u001b[0m     time\u001b[39m.\u001b[39;49msleep(interval)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "\n",
    "class TwitterScraper:\n",
    "    def __init__(self, api_key, api_key_secret, access_token, access_token_secret):\n",
    "        self.api = self.authenticate(api_key, api_key_secret, access_token, access_token_secret)\n",
    "        self.tweet_ids = set()  # Set to hold tweet ids\n",
    "\n",
    "    @staticmethod\n",
    "    def authenticate(api_key, api_key_secret, access_token, access_token_secret):\n",
    "        authenticator = tweepy.OAuthHandler(api_key, api_key_secret)\n",
    "        authenticator.set_access_token(access_token, access_token_secret)\n",
    "        return tweepy.API(authenticator, wait_on_rate_limit=True)\n",
    "\n",
    "    def get_tweets(self, search_terms, count):\n",
    "        tweets = tweepy.Cursor(\n",
    "            self.api.search_tweets,\n",
    "            q=search_terms,\n",
    "            lang='en',\n",
    "            tweet_mode='extended',\n",
    "        ).items(count)\n",
    "\n",
    "        data = []\n",
    "        for tweet in tweets:\n",
    "            if tweet.id not in self.tweet_ids:\n",
    "                self.tweet_ids.add(tweet.id)\n",
    "                tweet_dict = tweet._json\n",
    "                user = tweet_dict.pop('user', {})\n",
    "                for key, value in user.items():\n",
    "                    tweet_dict[f'user_{key}'] = value\n",
    "                data.append(tweet_dict)\n",
    "        print(f\"Fetched {len(data)} new tweets\")  # Print the number of new tweets fetched\n",
    "        return data\n",
    "\n",
    "\n",
    "    @staticmethod   \n",
    "    def save_to_csv(data, filepath):\n",
    "        df = pd.DataFrame(data)\n",
    "        if not df.empty:  # Check if the DataFrame is not empty\n",
    "            df['created_at'] = df['created_at'].apply(lambda x: parse(x).strftime('%Y-%m-%d %H:%M:%S'))\n",
    "            df.to_csv(filepath, mode='a', index=False)  # Add mode='a' to append to the existing file\n",
    "        else:\n",
    "            print(\"No new tweets fetched\")\n",
    "\n",
    "\n",
    "    def fetch_and_save(self, search_terms, count, filepath, interval=60, max_tweets=50000):\n",
    "        total_tweets = 0\n",
    "        while total_tweets < max_tweets:\n",
    "            tweets = self.get_tweets(search_terms, count)\n",
    "            self.save_to_csv(tweets, filepath)\n",
    "            total_tweets += len(tweets)\n",
    "            if total_tweets < max_tweets:\n",
    "                time.sleep(interval)\n",
    "\n",
    "# Credentials\n",
    "api_key = ''\n",
    "api_key_secret = ''\n",
    "access_token_secret = ''\n",
    "access_token = ''\n",
    "\n",
    "# Instantiate the scraper\n",
    "scraper = TwitterScraper(api_key, api_key_secret, access_token, access_token_secret)\n",
    "\n",
    "# Set parameters\n",
    "search_terms = '(\"Will Smith\" OR \"Chris Rock\" OR \"Jada Pinkett Smith\" OR \"Oscars\" OR \"Academy Awards\" OR \"slap\" OR \"#AcademyAwards\" OR \"#WillSmith\" OR \"#ChrisRock\" OR \"@jadapsmith\" OR \"@TheAcademy\" OR \"Best Actor award\")'\n",
    "count = 100\n",
    "\n",
    "# Define the filename\n",
    "now = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "directory = \"./data/\"  # data directory in the current directory\n",
    "filename = f\"tweets_{now}.csv\"\n",
    "filepath = os.path.join(directory, filename)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Start fetching and saving tweets\n",
    "scraper.fetch_and_save(search_terms, count, filepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
